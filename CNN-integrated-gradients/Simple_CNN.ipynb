{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SBhmsvbdrS9"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MFO7oytsYB_L"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, GlobalAveragePooling1D, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare your positive and negative dataset that will be loaded here\n",
    "\n",
    "the model is strictly using only sequences of length 500 -> longer sequences are discarded, shorter are padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i4RWxGBL3QU7"
   },
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv(\"Rossmann_unknotted.csv\")[[\"seq\"]]\n",
    "df_neg[\"label\"] = 0\n",
    "\n",
    "df_pos = pd.read_csv(\"SPOUT_knotted.csv\", sep=';')[[\"seq\"]]\n",
    "df_pos[\"label\"] = 1\n",
    "df_pos\n",
    "\n",
    "df_merged = pd.concat([df_neg, df_pos], ignore_index=True, sort=False).sample(frac=0.5, random_state=42).rename(columns={\"seq\": \"sequence\", \"label\": \"label\"})\n",
    "# delete long sequences\n",
    "df_merged = df_merged[df_merged['sequence'].str.len() <= 500]\n",
    "# padd shorter sequences\n",
    "df_merged['sequence'] = df_merged['sequence'].apply(lambda x: x + (500 - len(x))*'X')\n",
    "\n",
    "df_train, df_test = train_test_split(df_merged, test_size=0.2, random_state=42)\n",
    "del(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MSUVeyHtYb6C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 13:08:13.376188: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-31 13:08:14.544249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43662 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:a3:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "nucleo_dic = {\n",
    "    \"A\": 0,\n",
    "    \"R\": 1,\n",
    "    \"N\": 2,\n",
    "    \"D\": 3,\n",
    "    \"C\": 4,\n",
    "    \"Q\": 5,\n",
    "    \"E\": 6,\n",
    "    \"G\": 7,\n",
    "    \"H\": 8,\n",
    "    \"I\": 9,\n",
    "    \"L\": 10,\n",
    "    \"K\": 11,\n",
    "    \"M\": 12,\n",
    "    \"F\": 13,\n",
    "    \"P\": 14,\n",
    "    \"S\": 15,\n",
    "    \"T\": 16,\n",
    "    \"W\": 17,\n",
    "    \"Y\": 18,\n",
    "    \"V\": 19,\n",
    "    \"X\": 20\n",
    "}\n",
    "\n",
    "\n",
    "dataset_train = df_train['sequence'].tolist()\n",
    "labels_train = np.array(df_train['label'])\n",
    "# numericalize using the dictionary\n",
    "dataset_ordinal_train = [[nucleo_dic[letter] for letter in sequence] for sequence in dataset_train]\n",
    "# translate number values to one-hot vectors\n",
    "dataset_onehot_train = tf.one_hot(dataset_ordinal_train, depth=21)\n",
    "del(dataset_ordinal_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d6NicFJVY2h2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = df_test['sequence'].tolist()\n",
    "labels_test = np.array(df_test['label'])\n",
    "# we use the same nucleo_dic as on the example before\n",
    "dataset_ordinal_test = [[nucleo_dic[letter] for letter in sequence] for sequence in dataset_test]\n",
    "dataset_onehot_test = tf.one_hot(dataset_ordinal_test, depth=21)\n",
    "del(dataset_ordinal_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsCkvaT_dxDa"
   },
   "source": [
    "## Model\n",
    "\n",
    "Adapted model from this [paper](https://www.frontiersin.org/articles/10.3389/fgene.2020.568546/full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3hQFWf3uZWG6"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Conv1D(32, kernel_size=8, data_format='channels_last', activation='relu', input_shape=(500,21)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(),\n",
    "        Conv1D(16, kernel_size=8, data_format='channels_last', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(),\n",
    "        Conv1D(4, kernel_size=8, data_format='channels_last', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YROigDsQZ8Pd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 493, 32)           5408      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 493, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 246, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 239, 16)           4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 239, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 119, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 112, 4)            516       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 112, 4)            16        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 56, 4)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 56, 4)             0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 10,249\n",
      "Trainable params: 10,145\n",
      "Non-trainable params: 104\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-JO2cahgodo"
   },
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2o83ueDZv59",
    "outputId": "a4f7d470-5152-4971-9670-eb0363d39835"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 12:53:21.550446: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 12:53:23.146568: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2022-10-31 12:53:24.729067: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-31 12:53:24.729967: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-31 12:53:24.730001: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2022-10-31 12:53:24.730749: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-31 12:53:24.730846: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-10-31 12:53:26.300710: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 [==============================] - 9s 7ms/step - loss: 0.3547 - accuracy: 0.8796 - val_loss: 1.0417 - val_accuracy: 0.3899\n",
      "Epoch 2/10\n",
      "496/496 [==============================] - 3s 5ms/step - loss: 0.0886 - accuracy: 0.9845 - val_loss: 0.0595 - val_accuracy: 0.9878\n",
      "Epoch 3/10\n",
      "496/496 [==============================] - 3s 6ms/step - loss: 0.0352 - accuracy: 0.9951 - val_loss: 0.0336 - val_accuracy: 0.9933\n",
      "Epoch 4/10\n",
      "496/496 [==============================] - 3s 6ms/step - loss: 0.0199 - accuracy: 0.9974 - val_loss: 0.0258 - val_accuracy: 0.9937\n",
      "Epoch 5/10\n",
      "496/496 [==============================] - 3s 6ms/step - loss: 0.0126 - accuracy: 0.9987 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
      "Epoch 6/10\n",
      "496/496 [==============================] - 3s 5ms/step - loss: 0.0091 - accuracy: 0.9990 - val_loss: 0.0355 - val_accuracy: 0.9886\n",
      "Epoch 7/10\n",
      "496/496 [==============================] - 2s 5ms/step - loss: 0.0075 - accuracy: 0.9989 - val_loss: 0.0382 - val_accuracy: 0.9874\n",
      "Epoch 8/10\n",
      "496/496 [==============================] - 3s 5ms/step - loss: 0.0054 - accuracy: 0.9992 - val_loss: 0.0242 - val_accuracy: 0.9924\n",
      "Epoch 9/10\n",
      "496/496 [==============================] - 3s 5ms/step - loss: 0.0037 - accuracy: 0.9996 - val_loss: 0.0109 - val_accuracy: 0.9968\n",
      "Epoch 10/10\n",
      "496/496 [==============================] - 3s 6ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.0103 - val_accuracy: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd738055910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset_onehot_train,\n",
    "    labels_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bncLlx3PZ7GJ"
   },
   "outputs": [],
   "source": [
    "model.save(\"cnn_10epochs.h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aJdOi11WhIhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 493, 32)           5408      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 493, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 246, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 239, 16)           4112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 239, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 119, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 112, 4)            516       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 112, 4)            16        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 56, 4)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 56, 4)             0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 10,249\n",
      "Trainable params: 10,145\n",
      "Non-trainable params: 104\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('cnn_10epochs.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 13:08:56.847551: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-31 13:08:57.980664: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2022-10-31 13:08:59.491743: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-31 13:08:59.492919: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-31 13:08:59.492949: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2022-10-31 13:08:59.493493: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-31 13:08:59.493564: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.009760988876223564\n",
      "Test accuracy: 0.9969120621681213\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(dataset_onehot_test, labels_test, verbose = 0) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
