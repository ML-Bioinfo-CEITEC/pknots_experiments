{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2a2ebe-ddad-4772-a53b-98bed9a60a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f983d-4b63-47bd-b839-dad26e5a560a",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c54551f-b5e9-4e21-ade9-dbede0bc2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(32576, 512)\n",
    "        self.lin2 = nn.Linear(512, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.loss = nn.functional.binary_cross_entropy\n",
    " \n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.batchnorm(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.flat(X)\n",
    "        X = self.lin1(X)\n",
    "        X = self.lin2(X)\n",
    "        X = self.sigmoid(X)\n",
    "        return X\n",
    "    \n",
    "    def train_model(self, dataset, epochs):  \n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            with tqdm(dataset, unit=\"batch\") as tepoch:\n",
    "                for inputs, targets in tepoch:\n",
    "                    \n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                    tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "                    \n",
    "                    # clear the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # compute the model output\n",
    "                    yhat = self(inputs.float())\n",
    "                    # calculate accuracy\n",
    "                    correct = (torch.round(yhat) == targets).sum().item()\n",
    "                    accuracy = correct / len(inputs)\n",
    "                    # calculate loss\n",
    "                    loss = self.loss(yhat, targets.float())\n",
    "                    # credit assignment\n",
    "                    loss.backward()\n",
    "                    # update model weights\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "                \n",
    "    def test(self, dataloader):\n",
    "        model.eval()\n",
    "        predictions, actuals = list(), list()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                \n",
    "                # evaluate the model on the test set\n",
    "                yhat = self(inputs.float())\n",
    "                yhat = yhat.cpu().detach().numpy()\n",
    "                actual = targets.numpy()\n",
    "                # reshape for stacking\n",
    "                actual = actual.reshape((len(actual), 1))\n",
    "                yhat = yhat.reshape((len(yhat), 1))\n",
    "                # store\n",
    "                predictions.append(yhat)\n",
    "                actuals.append(actual)\n",
    "        predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
    "        print(\"Predictions: \", predictions[:10])\n",
    "        print(\"Real labels: \", actuals[:10])\n",
    "        # calculate accuracy\n",
    "        pred_label = np.round(predictions)\n",
    "        acc = metrics.accuracy_score(actuals, pred_label)\n",
    "        f1 = metrics.f1_score(actuals, pred_label, average='binary', zero_division=0)\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(actuals, predictions)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "        print(f\"Test metrics: \\n Accuracy: {float(acc):>6f}, F1 score: {float(f1):>6f}, AUPRC: {float(auprc):>6f}\\n\")\n",
    "        return acc, f1, auprc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d59859-8667-4e66-8a81-3cac40542327",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bc4e07-a0a8-4c12-8960-ebb8e48379ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.X = np.expand_dims(df.drop(columns=['label']), axis=1)\n",
    "        self.y = np.expand_dims(df['label'].to_numpy(), axis=1)\n",
    "        self.len = len(df)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def prepare_dataset(path):\n",
    "    # prepare train and test df dataset\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(columns=['seq'])\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dffddf7-c290-4e6d-b8d3-6db07062f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = prepare_dataset(\"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22f56ebe-1a7d-490f-a69f-94c51d7a5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 198755\n",
      "Test dataset size: 49689\n",
      "\n",
      "        label        f0        f1        f2        f3        f4        f5  \\\n",
      "195351      1  0.004589 -0.013615 -0.003538  0.005501  0.007457  0.008011   \n",
      "205311      1  0.004341 -0.001395  0.004811  0.004091  0.004792 -0.000042   \n",
      "2620        0  0.003129 -0.012673 -0.010728  0.003278  0.029388  0.016654   \n",
      "96580       0 -0.000483 -0.003341 -0.000792 -0.002914  0.002960  0.005678   \n",
      "171204      1 -0.005718 -0.009920 -0.006310 -0.001162  0.002309  0.001646   \n",
      "\n",
      "              f6        f7        f8  ...     f1014     f1015     f1016  \\\n",
      "195351 -0.001625 -0.017335  0.004249  ...  0.007692 -0.022566 -0.005259   \n",
      "205311 -0.004655 -0.006764 -0.000739  ...  0.003729 -0.010219 -0.008266   \n",
      "2620    0.016626 -0.019582 -0.001566  ...  0.002420 -0.027691 -0.022426   \n",
      "96580   0.002523 -0.002995 -0.002277  ...  0.001476 -0.002968 -0.002744   \n",
      "171204  0.004880 -0.012419  0.001810  ...  0.015332 -0.008871 -0.009826   \n",
      "\n",
      "           f1017     f1018     f1019     f1020     f1021     f1022     f1023  \n",
      "195351 -0.013545 -0.026974  0.012122 -0.010111 -0.005801 -0.001414 -0.001666  \n",
      "205311 -0.002788 -0.006637 -0.001873 -0.003918 -0.000278 -0.004456 -0.001767  \n",
      "2620   -0.011857 -0.033202 -0.014962 -0.022277 -0.001918 -0.013450  0.001564  \n",
      "96580  -0.001575 -0.004215  0.002143 -0.004380 -0.001378 -0.003826  0.001983  \n",
      "171204 -0.008161 -0.008446  0.005326 -0.002585 -0.007217 -0.006790  0.003817  \n",
      "\n",
      "[5 rows x 1025 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_df)}\")\n",
    "print(f\"Test dataset size: {len(test_df)}\\n\")\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0d59a43-ab38-40b3-a576-cbe7982e0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = EmbeddingDataset(train_df)\n",
    "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True)\n",
    "test_dset = EmbeddingDataset(test_df)\n",
    "test_loader = DataLoader(test_dset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c5e1d8-61b6-4f19-8a6f-0752f7c0c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Run on GPU or CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5312c2f-2c65-4d94-b93b-95364b3d3215",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a11a92-0728-495c-b18b-f11062608546",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4bd7a2b-7459-4ab0-9d00-3cd40881de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(7,), stride=(1,))\n",
      "  (batchnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (lin1): Linear(in_features=32576, out_features=512, bias=True)\n",
      "  (lin2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede6086-fe96-4b11-a6cd-a1e9eecdfba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6212/6212 [00:33<00:00, 186.52batch/s, accuracy=100, loss=0.00112] \n",
      "Epoch 6: 100%|██████████| 6212/6212 [00:33<00:00, 186.38batch/s, accuracy=100, loss=0.00392] \n",
      "Epoch 7: 100%|██████████| 6212/6212 [00:34<00:00, 182.08batch/s, accuracy=100, loss=0.0241]  \n",
      "Epoch 8: 100%|██████████| 6212/6212 [00:34<00:00, 181.40batch/s, accuracy=100, loss=3.62e-5] \n",
      "Epoch 9:   7%|▋         | 424/6212 [00:02<00:33, 171.44batch/s, accuracy=100, loss=8.14e-5] "
     ]
    }
   ],
   "source": [
    "model.train_model(train_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6110189c-62a6-428c-8a03-168e82d48022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[1.2111097e-03]\n",
      " [9.9999428e-01]\n",
      " [6.3317369e-09]\n",
      " [3.9129428e-04]\n",
      " [1.3323086e-05]\n",
      " [8.8208942e-09]\n",
      " [9.9999416e-01]\n",
      " [1.6985595e-09]\n",
      " [2.0679631e-06]\n",
      " [9.9999595e-01]]\n",
      "Real labels:  [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "Test metrics: \n",
      " Accuracy: 0.997424, F1 score: 0.997731, AUPRC: 0.999885\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9974239771377971, 0.9977305769298961, 0.9998846468275826)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3894337-929f-40ca-a050-a290bc57d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"embedding_CNN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aca2b8-a32f-437f-9572-3fc7150e8b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:knots_ML]",
   "language": "python",
   "name": "conda-env-knots_ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
